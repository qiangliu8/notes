# 爬虫入门

## 爬虫在使用场景的分类

- **通用爬虫**

  抓取系统重要组成部分，抓取的是**一整张页面数据**。

- **聚焦爬虫**

  建立在通用爬虫的基础之上，抓取的是页面中特定的**局部内容**

- **增量式爬虫**

  检测网站中**数据更新**的情况，只会抓取网站中最新更新出来的数据。

## 知识概念

#### 反爬机制

门户网站，可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。

#### 反反爬策略

爬虫程序可以通过制定相关的策略或者技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站

#### robots.txt协议

君子协议：确定规定了网站中哪些数据可以被爬虫爬取，哪些不可以爬取。

#### http协议

概念：服务器与客服端进行数据交互的一种形式

##### 常用请求头信息：

**User-Agent**：请求载体的身份标识

**Connection**：请求完毕后，是断开连接还是保持连接

##### 常用响应头信息

**Content-Type**：服务器响应回客户端的数据类型

#### https协议

安全的超文本传输协议（数据加密）

##### 加密方式

- 对称密秘钥加密
- 非对称秘钥加密
- 证书秘钥加密



## requests模块

python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。

#### User-Agent

#UA：User-Agent（请求载体的身份标识）

#UA检测：门户网站的服务器会检测对应请求的载体身份标识，如果检测到请求的载体身份标识为某一款浏览器，

#说明该请求是一个正常的请求。但是，如果检测到请求的载体身份标识不是基于某一款浏览器的，则表示该请求

#为不正常的请求（爬虫），则服务器端就很有可能拒绝该次请求。

#### 作用

模拟浏览器发请求。

#### 如何使用（requests模块的编码流程）

1. 指定url
2. 发起请求
3. 获取响应数据
4. 持久化存储

#### 环境安装

```python
pip install requests
```

#### 实战编码

需求：爬取搜狗首页的页面数据

## 数据解析

